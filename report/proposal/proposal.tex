\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

% \usepackage{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{subfig}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{media9} 
\usepackage{caption}
% \usepackage{subcaption}
\usepackage{amsmath}
\usepackage{algorithm, algpseudocode}

\title{Zero Shot Generation of Vectorized Images}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Nitish Kulkarni \And Aditya Siddhant \And George Tan \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Use of Neural Networks for generation of Images has gained popularity in recent times.  Most of the work in this area has targeted the modelling of pixel images. Humans, however do not learn to draw images as matrix of pixels. They learn to represent abstract concepts present in an object using a sequence of strokes. We would therefore be trying to address the challenge of generating images or rather sketches in a vectorized manner given a pixel image of the object. Our goal would be train a generative model that learns to draw and generalize abstract concepts in a manner indistinguishable from humans. We refer to this as `Zero Shot' because the problem entails generation different sketch representations of previously unseen objects in pixel images during test time.
\end{abstract}

\section{Related Work}
Although Free hand sketches have not received as much attention as pixel images, there has been quite a lot of work in recognition of Free Hand Sketches using Deep Neural Networks.\cite{rcp1} \cite{qdpaper} Some works have tried to complete the sketch conditioned on first few strokes\ and try to build a system for guiding the free-form drawing of objects\cite{qdpaper} \cite{rcp2} Other works have tried to retrieve the pixel image of an object given the sketch.\cite{sketchydb}. To the best of our knowledge there is no work that has used Deep Learning Approach to generate vectorized sketches using a pixel image.

\section{Proposed Approach}

Generative modelling of images has had recent success with  Generative Adversarial Networks (GAN) \cite{GAN}, a framework to estimate generative models with an adversarial process. GANs have been demonstrated to generate images of hand-written digits from MNIST \cite{MNIST}, faces from the Toronto Face Database \cite{TFD}, and various objects from CIFAR \cite{CIFAR}. We would like to explore the possible extension of this GAN framework to generate sketches given the pixel image of the object. In order to to learn the sequence of strokes in a sketch to imitate human-like behavior, we propose the use of Recurrent Neural Networks. Since Long Short-Term Memory (LSTM) \cite{LSTM} memory cells and Gated Recurrent Units (GRU) \cite{GRU} are well-suited to learn and predict on time series data, the GAN framework can be extended with recurrent neural networks (RNN) to generate the strokes of an object's sketch. In this case our adversary would try to distinguish RNN generated sketches from human drawn ones and the feedback would be used to train the parameters of RNN model. We plan to feed the encoding of pixel image into this RNN so that the stroke generation is conditioned on the pixel image.  We are working on the mathematical formulation of the problem.

\section{Datasets}
We would be using the Quick Draw Dataset \cite{quickdraw} to train and evaluate our model. The dataset consists of over 50 million drawings of 200 objects contributed by over 15 million players who played Googleâ€™s drawing game, "Quick, Draw!". The dataset also captured the players' strokes and the sequence of the strokes to create each drawing. Right now, we are planning on using 100 categories with 10k sketches each for training and validation and 25 categories for testing. This might change depending on the computational complexity of our method and the resources that are available to us. In addition to this, we would be either using the Sketchy Database \cite{sketchydb} or crawl the web for pixel images corresponding to the categories in Quick Draw Dataset.

{\small
\bibliographystyle{abbrvnat}
\bibliography{egbib}
}

\end{document}